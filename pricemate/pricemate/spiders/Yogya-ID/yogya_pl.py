import json
import re
import time
from parsel import Selector
from urllib.parse import quote
import scrapy
import os, sys
import subprocess
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from pricemate.spiders.base_spider import PricemateBaseSpider

def load_cookies_headers(filepath="cookies_and_headers.txt"):
    """
    Load cookies and headers from a .txt file generated by the Selenium script.
    The file must define variables named `cookies` and `headers`.
    """
    data = {}
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Cookies/headers file not found: {filepath}")
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    exec(content, data)
    cookies = data.get("cookies", {})
    headers = data.get("headers", {})
    return cookies, headers


class YogyaPlSpider(PricemateBaseSpider):
    name = "yogya_pl"

    def __init__(self, retailer, region, *args, **kwargs):
        super().__init__(retailer=retailer, region=region, *args, **kwargs)

    def start_requests(self):
        file_path = "cookies_and_headers.txt"
        selenium_script = "selenium_get_cookies_headers.py"
        # ✅ Check if cookies file exists first
        if not os.path.exists(file_path):
            print(f"{file_path} not found — running Selenium script...")
            subprocess.run([sys.executable, selenium_script], check=True)
        else:
            # Get the last modified time of the file
            last_modified_time = os.path.getmtime(file_path)
            current_time = time.time()

            # ✅ Check if cookies were modified in the last hour
            if current_time - last_modified_time <= 3600:
                print(f"{file_path} was updated in the last hour.")
            else:
                print(f"{file_path} was NOT updated in the last hour.")
                # ✅ Run Selenium script to refresh cookies
                print("Refreshing cookies using Selenium...")
                subprocess.run([sys.executable, selenium_script], check=True)

        # ✅ Load cookies and headers after confirming freshness
        cookies, headers = load_cookies_headers(file_path)
        docs = self.category_input.find({
            "retailer": self.retailer,
            "region": self.region,
            "Status": "Pending"
        })

        for doc in docs:
            url = doc["url"]
            hash_id = doc.get("_id")
            match = re.search(r"/supermarket/([^/]+)/category", url)

            slug = match.group(1)

            current_proxy = '2c6ea6e6d8c14216a62781b8f850cd5b'

            proxy_host = "api.zyte.com"
            proxy_port = "8011"
            proxy_auth = f"{current_proxy}:"

            proxy_url = f"https://{proxy_auth}@{proxy_host}:{proxy_port}"


            meta = {
                "proxy": proxy_url,
                "url": url,
                "_id": hash_id,
                "slug":slug,
                "filename": f"{slug}_page.html",
                "should_be": ["html"]
            }
            yield scrapy.Request(
                url,
                cookies = cookies,
                headers=headers,
                callback=self.parse_pl,
                meta=meta
            )

    def parse_pl(self, response):
        meta = response.meta
        cookies, headers = load_cookies_headers("cookies_and_headers.txt")
        doc_id = meta.get("_id")
        slug = meta.get("slug")
        if not response.xpath('//div[@class="row mt-3"]'):
            self.logger.error(f"Invalid page structure: {response.url}")
            return

        category_id = response.xpath('//div[@id="category"]/@data-category-id').get()
        if not category_id:
            self.logger.error(f"No category_id found for {response.url}")
            return
        self.logger.info(f"Found category_id: {category_id}")

            # extract product links
        hrefs = response.xpath('//div[@class="product-item box-shadow-light"]//a/@href').getall()
        unique_hrefs = list(dict.fromkeys(hrefs))

        for link in unique_hrefs:
            pdp_url = link
            product_hash = self.generate_hash_id(pdp_url, self.retailer, self.region)

            item = {
                "_id": product_hash,
                "ProductURL": pdp_url,
                "Status": "Pending",
                "retailer": self.retailer,
                "region": self.region,
            }
            self.save_product(item)
            self.category_input.update_one(
                {"_id": doc_id},
                {"$set": {"Status": "Done"}}
            )
            self.logger.info(f"Product URL: {pdp_url}")


        payload = {
            "current_page": 2,
            "brands": [],
            "category_id": category_id
        }
        yield scrapy.Request(
            url="https://supermarket.yogyaonline.co.id/load-more-product",
            method="POST",
            body=json.dumps(payload),
            cookies=cookies,
            headers={**headers, "Content-Type": "application/json"},
            callback=self.parse_ajax,
            meta={"page": 2, "category_id": category_id,"doc_id":doc_id,"filename": f"{slug}_{category_id}_page.html",
                "should_be": ["html"]}
        )

    def parse_ajax(self, response):
        """Handle paginated product loading via AJAX API"""
        meta = response.meta
        cookies, headers = load_cookies_headers("cookies_and_headers.txt")
        doc_id = meta.get("doc_id")
        slug = meta.get("slug")
        category_id = meta.get("category_id")
        data = json.loads(response.text)
        if not data.get("status"):
            self.logger.error(f"AJAX failed: {response.text}")
            return

        # extract products from returned HTML
        sel = Selector(text=data.get("html", ""))
        hrefs = sel.xpath('//div[@class="product-item box-shadow-light"]//a/@href').getall()
        unique_hrefs = list(dict.fromkeys(hrefs))

        for link in unique_hrefs:
            pdp_url = link
            product_hash = self.generate_hash_id(pdp_url, self.retailer, self.region)

            item = {
                "_id": product_hash,
                "ProductURL": pdp_url,
                "Status": "Pending",
                "retailer": self.retailer,
                "region": self.region,
            }
            self.save_product(item)
            self.category_input.update_one(
                {"_id": doc_id},
                {"$set": {"Status": "Done"}}
            )
            self.logger.info(f"Product URL: {pdp_url}")

        # check pagination
        page = response.meta["page"]
        total_page = data.get("total_page", page)
        if page < total_page:
            next_payload = {
                "current_page": page + 1,
                "brands": [],
                "category_id": response.meta["category_id"]
            }
            yield scrapy.Request(
                url="https://supermarket.yogyaonline.co.id/load-more-product",
                method="POST",
                body=json.dumps(next_payload),
                cookies = cookies,
                headers={**headers, "Content-Type": "application/json"},
                callback=self.parse_ajax,
                meta={"page": page + 1, "category_id": response.meta["category_id"],"filename": f"{slug}_{category_id}_{page}_page.html",
                "should_be": ["html"]}
            )


    def close(self, reason):
        self.mongo_client.close()

if __name__ == '__main__':
    from scrapy.cmdline import execute
    execute("scrapy crawl yogya_pl -a retailer=yogya-id -a region=id -a Type=eshop -a RetailerCode=yogya_id".split())